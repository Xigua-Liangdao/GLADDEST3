# Training settings
n_epochs: 1000
batch_size: 8
lr: 0.0002
clip_grad: null          # float, null to disable
save_model: True
num_workers: 0
ema_decay: 0           # 'Amount of EMA decay, 0 means off. A reasonable value  is 0.999.'
progress_bar: false
weight_decay: 1e-12
optimizer: adamw # adamw,nadamw,nadam => nadamw for large batches, see http://arxiv.org/abs/2102.06356 for the use of nesterov momentum with large batches
seed: 0

time_distortion: "identity"  # 'identity', 'cosine', polyinc, polydec

# Optional trainer limits (set to a small int or fraction to only run a few batches)
limit_train_batches: null   # int for number of batches, or float in (0,1] for fraction of dataset; null => full
limit_val_batches: null
limit_test_batches: null
max_steps: null             # int; override total steps; null => disabled
accumulate_grad_batches: 1  # effective batch size = batch_size * accumulate_grad_batches
